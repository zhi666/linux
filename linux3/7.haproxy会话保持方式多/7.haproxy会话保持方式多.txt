

haproxy	   支持4层，7层负载均衡，反向代理,会话保持，大并发。
LVS	   稳定，效率高，四层调度。不支持7层的内容分发或过滤。不支持会话保持。
nginx	   支持七层调度，现在也有开发的新的模块来扩展调度相关的功能。在会话保持，内容分发过滤方面比haproxy相比要差



www.haproxy.com



下图中haproxy用了两个网段(这里模拟内外网),实际时也可以只用一个网卡(只有内网网卡),公网IP在前端就可以了.这里尽量用两个内网来做，防止桥接网络IP冲突. 



			   客户端（宿主机) 192.168.2.x	
			     |			
			     |	
		外网	     |		192.168.2.65
			  haproxy   
		内网	     |		192.168.224.10
			     |			   
			     |
			web1     	 web2
		192.168.224.11	       192.168.224.12




实验前准备: (centos7.3平台)
1，配置主机名和主机名互相绑定
# hostnamectl set-hostname --static server.example.com

# vim /etc/hosts
192.168.224.10	server.example.com
192.168.224.11	client1.example.com
192.168.224.12   client2.example.com

2,静态ip
3,关闭iptables，selinux




第一步：
1,客户端准备
客户端有firefox和curl就可以了


2,后台web服务器准备　
把web1和web2装好httpd，然后启动起来，分别做一个不同内容的主页方便验证
web1上做：
yum install httpd httpd-devel -y
systemctl start httpd
systemctl enable httpd
echo web1 > /var/www/html/index.html

web2上做：
yum install httpd* -y
systemctl start httpd
systemctl enable httpd
echo web2 > /var/www/html/index.html




第二步：
在haproxy服务器上安装haproxy
# yum install haproxy -y

/usr/share/doc/haproxy-1.5.18/haproxy-en.txt	--参数文档
/etc/haproxy/haproxy.cfg	--主配置文件



第三步:
配置haproxy

配置结构介绍:
global 
	全局配置参数（主要配置服务用户，pid,socket,进程，chroot等)

defaults
	负载调度相关的全局配置　（调度模式，调度超时时间，调度算法，健康检查等等。配置在这个段，那么就默认对后面的listen,frontend,backend都生效)

frontend	
	处理前台接收的请求，可以在这个段配置acl进行七层调度等，指定调到对应的backend
backend
	最终调度的realserver相关配置

listen (就是frontend和backend的综合体)

先备份
cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak
# grep -v '#' /etc/haproxy/haproxy.cfg	 --下面是我的配置示例（global,defaults都基本没有调整，优化相关参数请参考文档和实际生产环境做调整)

global
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    stats socket /var/lib/haproxy/stats

defaults
    mode                    http			#支持7层负载均衡，如果改成tcp就只支持4层负载均衡
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s


listen  192.168.2.65 *:80
        balance roundrobin
        server client1.example.com 192.168.224.11:80
        server client2.example.com 192.168.224.12:80

systemctl start haproxy
systemctl enable haproxy

客户端　curl 192.168.2.65 测试
-------------------------------------------------------
把上面的listen配置段改成下面的frontend和backend，效果一样

frontend  192.168.2.65 *:80
        default_backend webs

backend webs
	balance roundrobin
        server client1.example.com 192.168.224.11:80
        server client2.example.com 192.168.224.12:80

--------------------------------------------------------


systemctl restart haproxy
systemctl enable haproxy


第四步:
客户端　curl 192.168.2.65 测试

结果为rr轮循web1,web2


===================================================================================
查看haproxy状态页面，
实际这个页面显示的内容为haproxy的状态页面，不是后台web的显示内容。下面参数中之所以有两台web服务器，是为了统计web服务器的状态。

--配置文件listen配置段加上stats的四行(将listen替换为下面)
listen  192.168.2.65 *:80
	stats uri /haproxy-stats 	--指定访问的路径	
        stats realm Haproxy\ statistics	--指定统计信息提示
        stats auth name:123		--需要验证的用户名和密码才能登录查看
	stats hide-version		--隐藏客户端访问统计页面时的haproxy版本号
        balance roundrobin   
        server client1.example.com 192.168.224.11:80
        server client2.example.com 192.168.224.12:80

systemctl reload haproxy.service

--客户端使用下面的去访问
http://192.168.2.41/haproxy-stats



或者换成下面的配置，效果一样

frontend  192.168.2.65 *:80
	  default_backend servers

backend servers
	stats uri /haproxy-stats
        stats realm Haproxy\ statistics
        stats auth li:li123
	stats hide-version	
        balance roundrobin
        server client1.example.com 192.168.224.11:80
        server client2.example.com 192.168.224.12:80


-客户端使用下面的去访问
http://192.168.2.65/haproxy-stats

＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

关于haproxy日志


vim /etc/rsyslog.conf
$ModLoad imudp
$UDPServerRun 514	--打开这两句的注释，表示udp协议514端口接收远程日志（haproxy日志做到127.0.0.1的本地，也要按远程日志做法来做)

local2.*      /var/log/haproxy.log	--加上这一句，表示local2日志设备的所有级别日志都会记录到后面的文件路径中(local2是和haproxy里的配置对应的。这条语句加在“$UDPServerRun 514”之后）
local2  指的是 /etc/haproxy/haproxy.cfg 里面 26 行

systemctl restart rsyslog.service
tail -f /var/log/haproxy.log (默认此文件不存在，但是有用户访问网站，日志文件就开始生成，并开始输出日志)


＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝


haproxy的健康检查功能和日志处理。 原理是在指定位置放置文件，如果文件存在则认为服务器是健康，否则认为web服务不可用。

vim /etc/haproxy/haproxy.cfg 替换listen整段
listen  192.168.2.65 *:80
    stats uri /haproxy-stats
        stats realm Haproxy\ statistics
        stats auth li:li123
    stats hide-version
        balance roundrobin
        option forwardfor		--日志forward，让后台web记录客户端的IP，而不是haproxy的IP
        option httpchk HEAD /check.txt HTTP/1.0	--健康检查功能如果后台web服务器家目录中没有check.txt文件，则表示后台web挂掉；此版本要使用http的1.0版，1.1版还不支持
        server client1.example.com 192.168.224.11:80 check inter 2000 rise 2 fall 5
        server client2.example.com 192.168.224.12:80 check inter 2000 rise 2 fall 5	--加上检查的间隔2秒，rise 2是2次正确表示服务器可用；fall 5表示5次失败表示服务器不可用




创建健康检查文件
On Web1
touch /var/www/html/check.txt

On Web2:
touch /var/www/html/check.txt

systemctl reload haproxy  --刷新
服务正常
curl 192.168.224.10  正常显示

客户端验证健康检查：
尝试删除文件/var/www/html/check.txt，然后运行命令 curl 192.168.224.10，观察结果变化.结果为有check.txt文件的则可以被调，没有的就不可以.


日志问题1
在web服务器端
vim /var/log/httpd/access_log 
后台web里每２秒都会有一句healthcheck的日志，想删除他，方法如下


方法一:
写一个脚本就如下两句，定时去执行一次就可以了
sed -i '/check.txt\ HTTP\/1.0/d' /var/log/httpd/access_log 
note: sed -i 配合字符串末尾的‘d’，实现删除特定字符串

kill -HUP `cat /var/run/httpd/httpd.pid`   
(sed命令修改access_log会导致文件被httpd进程无法持续写入日志 kill -HUP让httpd重启服务)
(如果想要更改配置而不需停止并重新启动服务，请使用该命令。在对配置文件作必要的更改后，发出该命令以动态更新服务配置)


方法二:
httpd不记录检测日志:	--健康检查有大量的日志，指定不记录它. 
vim /etc/httpd/conf/httpd.conf, 搜索key word'CustomLog'，然后修改。(或者注释217行增加下面2行)
SetEnvIf Request_URI "^/check\.txt$" dontlog
CustomLog logs/access_log combined env=!dontlog


重启后端的web服务器
systemctl restart httpd




日志问题２:
查看后端web的access.log，客户端的正常访问日志的IP并不是实际客户端IP，而是haproxy的内网IP


解决方法如下：
方法一：
直接使用前端haproxy的日志


方法二：
后端apache日志处理   --为了让access.log显示客户端的IP，而不是haproxy调度器的IP
配置
vim /etc/httpd/conf/httpd.conf                   #196行
LogFormat "%{X-Forwarded-For}i %l %u %t \"%r\" %>s %b " combined   #可以只加这行  

CustomLog logs/access_log combined                  #结果加了这一行后问题一的配置参数又失效了。
--把原来的combined条目替换 (或者增加这行)    

=================================================================================================



haproxy会话保持

1,source算法（类似nginx的ip_hash算法或lvs的sh算法)

vim /etc/haproxy/haproxy.cfg 替换 listen段

listen  192.168.2.65 *:80
    stats uri /haproxy-stats
        stats realm Haproxy\ statistics
        stats auth li:li123
    stats hide-version
        balance source			--把roundrobin改为source
        option forwardfor
        option httpchk HEAD /check.txt HTTP/1.0
        server client1.example.com 192.168.224.11:80 check inter 2000 rise 2 fall 5
        server client2.example.com 192.168.224.12:80 check inter 2000 rise 2 fall 5

systemctl reload haproxy  --reload后客户端测试
测试结果为一个客户端的请求会转发到同一个web服务器上。（亲缘性保持）


2,使用cookie(对浏览器有效，对elinks无效）
vim /etc/haproxy/haproxy.cfg
listen  192.168.2.65 *:80
    stats uri /haproxy-stats
        stats realm Haproxy\ statistics
        stats auth li:li123
    stats hide-version
        balance roundrobin
        option forwardfor
        option httpchk HEAD /check.txt HTTP/1.0
        cookie web_cookie insert nocache     --web_cookie为名称；当客户端和HAProxy之间存在缓存时，建议将insert配合nocache一起使用
        server client1.example.com 192.168.224.11:80 cookie web1 check inter 2000 rise 2 fall 5
        server client2.example.com 192.168.224.12:80 cookie web2 check inter 2000 rise 2 fall 5	 web1和 web2为后端web的cookie名称，不要一样


systemctl reload haproxy  --reload后客户端测试
测试结果为：使用浏览器访问可以保持亲缘性会话，但是用curl工具则无法保持，因为curl不支持session



3,利用haproxy内置的stick table来实现会话保持。
stick table是haproxy的一个非常优秀的特性，这个表里面存储的是stickiness记录，stickiness记录了客户端和服务端1:1对应的引用关系。通过这个关系，haproxy可以将客户端的请求引导到之前为它服务过的后端服务器上，也就是实现了会话保持的功能。这种记录方式，俗称会话粘性(stickiness)，即将客户端和服务端粘连起来。

stick table中使用key/value的方式映射客户端和后端服务器，key是客户端的标识符，可以使用客户端的源ip(50字节)、cookie以及从报文中过滤出来的部分String。value部分是服务端的标识符。


由于每条stickiness记录占用空间都很小(平均最小50字节，最大166字节，由是否记录额外统计数据以及记录多少来决定占用空间大小)，使得即使在非常繁忙的环境下多个节点之间推送都不会出现压力瓶颈和网络阻塞(可以按节点数量、stickiness记录的大小和平均并发量来计算每秒在网络间推送的数据流量)。

它不像被人诟病的session复制(copy)，因为session复制的数据量比较大，而且是在各应用程序服务器之间进行的。而一个稍大一点的核心应用，提供服务的应用程序服务器数量都不会小，这样复制起来很容出现网络阻塞。

此外，stick table还可以在haproxy重启时，在同一个机器内新旧两个进程间进行复制，这是本地复制。当haproxy重启时，旧haproxy进程会和新haproxy进程建立TCP连接，将其维护的stick table推送给新进程。这样新进程不会丢失粘性信息，和其他节点也能最大程度地保持同步，使得其他节点只需要推送该节点重启过程中新增加的stickiness记录就能完全保持同步。

vim /etc/haproxy/haproxy.cfg    (替换listen块)
listen  192.168.2.65 *:80
	stick-table type ip size 1m expire 1m
        stick on src         --以源ip为key进行粘贴，size 1m表示能记录100W条，1分钟没请求粘贴过期					
        stats uri /haproxy-stats
        stats realm Haproxy\ statistics
        stats auth li:li123
        stats hide-version
    balance roundrobin
        option forwardfor
        option httpchk HEAD /check.txt HTTP/1.0
        server client1.example.com 192.168.224.11:80 check inter 2000 rise 2 fall 5
        server client2.example.com 192.168.224.12:80 check inter 2000 rise 2 fall 5


systemctl reload haproxy  --reload后客户端测试
注意：这里最好用listen格式写，用frontend,backend格式写的话，以源ip为key的参数不生效。
第二次做实验用frontend,backend格式写，以源ip为key的参数可以生效。
验证：
无论使用Curl还是浏览器都可以保持亲缘性会话。

==============================================================




例一:使用haproxy做动静分离或网站数据切分（七层调度)

vim /etc/haproxy/haproxy.cfg    (替换listen块)
frontend 192.168.2.65 *:80
    acl invalid_src src 192.168.2.x    --如果你要拒绝它访问，就把注释打开   #测试没有效果 src去掉就有效果
      block if invalid_src 

        acl url_static path_end .html .png .jpg .css .js      #url_static相当于变量名   #path_end表示以什么结束的文件，
        use_backend static if url_static                #static 为自定义名，静态文件
        default_backend dynamic                       #dynamic  为自定义名 其他类型

backend static
    balance roundrobin
    server client1.example.com 192.168.224.11:80 check inter 2000 rise 2 fall 5
	    
backend dynamic
    balance roundrobin
    server client2.example.com 192.168.224.12:80 check inter 2000 rise 2 fall 5


 systemctl restart haproxy

测试结果：
    .html .png .jpg .css .js 被认为是静态文件，都会转发到 224.11, 其他类型的文件请求都会被转发到224.12
测试方法：
    在web服务器上建立.txt类型的文件测试是否请求会被转发到224.12. 也可以请求不存在的文件，haproxy仍然会将请求转发到后台web。通过查看后台web日志可以判定请求被转发到了那个web. web日志查看命令： tail -f /var/log/httpd/access_log
echo web1-html > /var/www/html/1.html
echo web1-txt > /var/www/html/1.txt

echo web2-html > /var/www/html/1.html
echo web2-txt > /var/www/html/1.txt

例二: 实现网站切分
 vim /etc/haproxy/haproxy.cfg    (替换listen块)

frontend 192.168.2.65 *:80
	acl url_static  path_beg    /static /images /img
    acl url_static  path_end .html .png .jpg .css .js         #url_static相当于变量名
    
    acl host_static hdr_beg(host) -i img. video. download.    #host_static相当于变量名
    acl host_www    hdr_beg(host) -i www

  use_backend static if url_static               #static相当于变量名
  use_backend static if host_static                 
  use_backend dynamic if host_www

backend static                                              #static=上面的变量名
    balance roundrobin
    server client1.example.com 192.168.224.11:80 check inter 2000 rise 2 fall 5

backend dynamic
    balance roundrobin
    server client2.example.com 192.168.224.12:80 check inter 2000 rise 2 fall 5




在客户端编辑hosts
vim /etc/hosts
192.168.2.65    www.abc.com
192.168.2.65    img.abc.com

测试结果：
curl http://www.abc.com                 -> Web2
curl http://www.abc.com/static/abc.abc  -> Web1
curl http://192.168.2.65/1.html         -> Web1
curl http://img.abc.com/static/abc.abc  -> Web1

重点：
http://www.abc.com  因为是www开头，所以定义为动态内容
http://www.abc.com/static/abc.abc 尽管/static仍然由web2处理，
如果URL同时命中了多条ACL规则，那URL结尾所访问的文件类型优先级别最高
============================================================

Lvs, Haproxy, Nginx这几种软件的特点。

squid,,nginx cache可以做缓存加速

开源负载均衡
nginx
harpoxy
lvs

lvs  四层
nginx,haproxy  七层

抗并发   lvs >  haproxy  > nginx
应用范围 lvs最广
网络复杂度 lvs复杂
会话保持  haproxy会话保持的方式较多
负载均衡算法   lvs最多



=============================================================================